[{"UID":"paper7","abstract":"Text style transfer aims to change the style (e.g., sentiment, politeness) of a sentence while preserving its content. A common solution is the prototype editing approach, where stylistic tokens are deleted in the \u201cmask\u201d stage and then the masked sentences are infilled with the target style tokens in the \u201cinfill\u201d stage. Despite their success, these approaches still suffer from the content preservation problem. By closely inspecting the results of existing approaches, we identify two common types of errors: 1) many content-related tokens are masked and 2) irrelevant words associated with the target style are infilled. Our paper aims to enhance content preservation by tackling each of them. In the \u201cmask\u201d stage, we utilize a BERT-based keyword extraction model that incorporates syntactic information to prevent content-related tokens from being masked. In the \u201cinfill\u201d stage, we create a pseudo-parallel dataset and train a T5 model to infill the masked sentences without introducing irrelevant content. Empirical results show that our method outperforms the state-of-theart baselines in terms of content preservation, while maintaining comparable transfer effectiveness and language quality.","authors":["Wanzheng Zhu","Suma Bhat"],"code":"static/paperfiles/paper7_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994325254329618563","full_video":"https://player.vimeo.com/video/728950843","paper":"static/paperfiles/paper7_paper.pdf","session":"Oral Session 3","short_time":"15:00 EST","slides":"static/paperfiles/paper7_slides.pdf","start_time":"2023-07-21T15:00:00Z","time":"Friday 07/21 15:00 EST","title":"'Slow Service' $\\Cancel{\\Rightarrow}$ 'Great Food': Enhancing Content Preservation In Unsupervised Text Style Transfer","zoom":"https://colby.zoom.us/j/98590322530"},{"UID":"paper45","abstract":"We describe our multi-task learning based approach for summarization of real-life dialogues as part of the DialogSum Challenge shared task at INLG 2023. Our approach intends to improve the main task of abstractive summarization of dialogues through the auxiliary tasks of extractive summarization, novelty detection and language modeling. We conduct extensive experimentation with different combinations of tasks and compare the results. In addition, we also incorporate the topic information provided with the dataset to perform topic-aware summarization. We report the results of automatic evaluation of the generated summaries in terms of ROUGE and BERTScore.","authors":["Saprativa Bhattacharjee","Kartik Shinde","Tirthankar Ghosal","Asif Ekbal"],"code":"static/paperfiles/paper45_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994686042420555896","full_video":"https://player.vimeo.com/video/728956443","paper":"static/paperfiles/paper45_paper.pdf","poster":"static/paperfiles/paper45_poster.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"A Multi-Task Learning Approach For Summarization Of Dialogues","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper42","abstract":"In this paper, we describe our reproduction effort of the paper: Towards Best Experiment Design for Evaluating Dialogue System Output by Santhanam and Shaikh (2019) for the 2023 ReproGen shared task. We aim to produce the same results, using different human evaluators, and a different implementation of the automatic metrics used in the original paper. Although overall the study posed some challenges to reproduce (e.g. difficulties with reproduction of automatic metrics and statistics), in the end we did find that the results generally replicate the findings of Santhanam and Shaikh (2019) and seem to follow similar trends.","authors":["Anouck Braggaar","Fr\u00e9d\u00e9ric Tomas","Peter Blomsma","Saar Hommes","Nadine Braun","Emiel van Miltenburg","Chris Van der Lee","Martijn Goudbeek","Emiel Krahmer"],"code":"static/paperfiles/paper42_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994685743744172112","full_video":"https://player.vimeo.com/video/728956524","paper":"static/paperfiles/paper42_paper.pdf","poster":"static/paperfiles/paper42_poster.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"A Reproduction Study Of Methods For Evaluating Dialogue System Output: Replicating Santhanam And Shaikh (2019)","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper15","abstract":"Noisy channel models have been especially effective in neural machine translation (NMT). However, recent approaches like \u201cbeam search and rerank\u201d (BSR) incur significant computation overhead during inference, making realworld application infeasible. We aim to study if it is possible to build an amortized noisy channel NMT model such that when we do greedy decoding during inference, the translation accuracy matches that of BSR in terms of reward (based on the source-to-target log probability and the target-to-source log probability) and quality (based on BLEU and BLEURT). We attempt three approaches to train the new model: knowledge distillation, 1- step-deviation imitation learning, and Q learning. The first approach obtains the noisy channel signal from a pseudo-corpus, and the latter two approaches aim to optimize toward a noisy-channel MT reward directly. For all three approaches, the generated translations fail to achieve rewards comparable to BSR, but the translation quality approximated by BLEU and BLEURT is similar to the quality of BSRproduced translations. Additionally, all three approaches speed up inference by 1\u20132 orders of magnitude.","authors":["Richard Yuanzhe Pang","He He","Kyunghyun Cho"],"code":"static/paperfiles/paper15_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994325817221976124","full_video":"https://player.vimeo.com/video/728953169","paper":"static/paperfiles/paper15_paper.pdf","poster":"static/paperfiles/paper15_poster.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","slides":"static/paperfiles/paper15_slides.pdf","start_time":"2023-07-19T16:30:00Z","time":"Wednesday 07/19 16:30 EST","title":"Amortized Noisy Channel Neural Machine Translation","zoom":""},{"UID":"paper27","abstract":"We propose a novel application of prompting Pre-trained Language Models (PLMs) to generate analogies and study how to design effective prompts for two task settings: generating a source concept analogous to a given target concept (aka Analogous Concept Generation ACG), and generating an explanation of the similarity between a given pair of target concept and source concept (aka Analogous Concept Explanation or ACE). We found that it is feasible to prompt InstructGPT to generate meaningful analogies and the best prompts tend to be precise imperative statements especially with low temperature setting. We also systematically analyzed the sensitivity of the InstructGPT model to prompt design and temperature and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements). Further, we conducted human evaluation on 1.3k of the generated analogies and found that the quality of generations varies substantially by model size. The largest InstructGPT model can achieve human-level performance at generating meaningful analogies for a given target while there is still room for improvement on the ACE task.","authors":["Bhavya Bhavya","Jinjun Xiong","ChengXiang Zhai"],"discord":"https://discordapp.com/channels/991043733573218384/994326745383719042","full_video":"https://player.vimeo.com/video/728954607","paper":"static/paperfiles/paper27_paper.pdf","session":"Oral Session 3","short_time":"14:40 EST","slides":"static/paperfiles/paper27_slides.pdf","start_time":"2023-07-21T14:40:00Z","time":"Friday 07/21 14:40 EST","title":"Analogy Generation By Prompting Large Language Models: A Case Study Of Instructgpt","zoom":"https://colby.zoom.us/j/98590322530"},{"UID":"paper8","abstract":"Image captioning is the process of automatically generating a textual description of an image. It has a wide range of applications, such as effective image search, auto archiving and even helping visually impaired people to see. English image captioning has seen a lot of development lately, while Arabic image captioning is lagging behind. In this work, we developed and evaluated several Arabic image captioning models with well-established metrics on a public image captioning benchmark. We initialized all models with transformers pre-trained on different Arabic corpora. After initialization, we fine-tuned them with image-caption pairs using a learning method called OSCAR. OSCAR uses object tags detected in images as anchor points to significantly ease the learning of image-text semantic alignments. In relation to the image captioning benchmark, our best performing model scored 0.39, 0.25, 0.15 and 0.092 with BLEU-1,2,3,4 respectively1 , an improvement over previously published scores of 0.33, 0.19, 0.11 and 0.057. Beside additional evaluation metrics, we complemented our scores with human evaluation on a sample of our output. Our experiments showed that training image captioning models with Arabic captions and English object tags is a working approach, but that a pure Arabic dataset, with Arabic object tags, would be preferable.","authors":["Jonathan Emami","Pierre Nugues","Ashraf Elnagar","Imad Afyouni"],"code":"static/paperfiles/paper8_code.zip","data":"static/paperfiles/paper8_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994325334491136110","full_video":"https://player.vimeo.com/video/728950821","paper":"static/paperfiles/paper8_paper.pdf","poster":"static/paperfiles/paper8_poster.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","start_time":"2023-07-19T16:30:00Z","time":"Wednesday 07/19 16:30 EST","title":"Arabic Image Captioning Using Pre-Training Of Deep Bidirectional Transformers","zoom":""},{"UID":"paper29","abstract":"Decoding strategies play a crucial role in natural language generation systems. They are usually designed and evaluated in open-ended text-only tasks, and it is not clear how different strategies handle the numerous challenges that goal-oriented multimodal systems face (such as grounding and informativeness). To answer this question, we compare a wide variety of different decoding strategies and hyper-parameter configurations in a Visual Dialogue referential game. Although none of them successfully balance lexical richness, accuracy in the task, and visual grounding, our in-depth analysis allows us to highlight the strengths and weaknesses of each decoding strategy. We believe our findings and suggestions may serve as a starting point for designing more effective decoding algorithms that handle the challenges of Visual Dialogue tasks.","authors":["Amit Kumar Chaudhary","Alex J. Lucassen","Ioanna Tsani","Alberto Testoni"],"code":"static/paperfiles/paper29_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326994688933948","full_video":"https://player.vimeo.com/video/728954469","paper":"static/paperfiles/paper29_paper.pdf","poster":"static/paperfiles/paper29_poster.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","start_time":"2023-07-19T16:30:00Z","summary_video":"https://player.vimeo.com/video/728954525","time":"Wednesday 07/19 16:30 EST","title":"Are Current Decoding Strategies Capable Of Facing The Challenges Of Visual Dialogue?","zoom":""},{"UID":"paper3","abstract":"Ad creatives are ads served to users on a webpage, app, or other digital environments. The demand for compelling ad creatives surges drastically with the ever-increasing popularity of digital marketing. The two most essential elements of (display) ad creatives are the advertising message, such as headlines and description texts, and the visual component, such as images and videos. Traditionally, ad creatives are composed by professional copywriters and creative designers. The process requires significant human effort, limiting the scalability and efficiency of digital ad campaigns. This work introduces AUTOCREATIVE, a novel system to automatically generate ad creatives relying on natural language generation and computer vision techniques. The system generates multiple ad copies (ad headlines/description texts) using a sequence-to-sequence model and selects images most suitable to the generated ad copies based on heuristic-based visual appeal metrics and a text-image retrieval pipeline.","authors":["Vishakha Kadam","Yiping Jin","Bao-Dai Nguyen-Hoang"],"code":"static/paperfiles/paper3_code.zip","discord":"https://discordapp.com/channels/991043733573218384/996467827760439396","full_video":"https://player.vimeo.com/video/728948586","paper":"static/paperfiles/paper3_paper.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","slides":"static/paperfiles/paper3_slides.pdf","start_time":"2023-07-20T15:00:00Z","summary_video":"https://player.vimeo.com/video/728948619","time":"Thursday 07/20 15:00 EST","title":"Automated Ad Creative Generation","zoom":""},{"UID":"paper12","abstract":"We present a novel approach to generating news headlines in Finnish for a given news story. We model this as a summarization task where a model is given a news article, and its task is to produce a concise headline describing the main topic of the article. Because there are no openly available GPT-2 models for Finnish, we will first build such a model using several corpora. The model is then fine-tuned for the headline generation task using a massive news corpus. The system is evaluated by 3 expert journalists working in a Finnish media house. The results showcase the usability of the presented approach as a headline suggestion tool to facilitate the news production process.","authors":["Maximilain Koppatz","Khalid Alnajjar","Mika H\u00e4m\u00e4l\u00e4inen","Thierry Poibeau"],"code":"static/paperfiles/paper12_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994325603106947182","full_video":"https://player.vimeo.com/video/728952993","paper":"static/paperfiles/paper12_paper.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","slides":"static/paperfiles/paper12_slides.pdf","start_time":"2023-07-20T15:00:00Z","time":"Thursday 07/20 15:00 EST","title":"Automatic Generation Of Factual News Headlines In Finnish","zoom":""},{"UID":"paper35","abstract":"Code-Mixed text data consists of sentences having words or phrases from more than one language. Most multi-lingual communities worldwide communicate using multiple languages, with English usually one of them. Hinglish is a Code-Mixed text composed of Hindi and English but written in Roman script. This paper aims to determine the factors influencing the quality of Code-Mixed text data generated by the system. For the HinglishEval task, the proposed model uses multilingual BERT to find the similarity between synthetically generated and human-generated sentences to predict the quality of synthetically generated Hinglish sentences.","authors":["Shaz Furniturewala","Vijay Kumari","Amulya Ratna Dash","Hriday Kedia","Naman Ahuja","Yashvardhan Sharma"],"code":"static/paperfiles/paper35_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994684997321641994","full_video":"https://player.vimeo.com/video/728955769","paper":"static/paperfiles/paper35_paper.pdf","session":"GenChal","short_time":"12:00 EST","slides":"static/paperfiles/paper35_slides.pdf","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Bits Pilani At Hinglisheval: Quality Evaluation For Code-Mixed Hinglish Text Using Transformers","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper1","abstract":"This demo paper introduces BLAB reporter, a robot-journalist system covering the Brazilian Blue Amazon. The application is based on a pipeline architecture for Natural Language Generation, which offers daily reports, news summaries and curious facts in Brazilian Portuguese. By collecting, storing and analysing structured data from publicly available sources, the robot-journalist uses domain knowledge to generate, validate and publish texts in Twitter. Code and corpus are publicly available.","authors":["Yan Vianna Sym","Jo\u00e3o Gabriel Moura Campos","Fabio Cozman"],"code":"static/paperfiles/paper1_code.zip","discord":"https://discordapp.com/channels/991043733573218384/996467698332614666","full_video":"https://player.vimeo.com/video/730976334","paper":"static/paperfiles/paper1_paper.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","slides":"static/paperfiles/paper1_slides.pdf","start_time":"2023-07-19T16:30:00Z","summary_video":"https://player.vimeo.com/video/730976519","time":"Wednesday 07/19 16:30 EST","title":"Blab Reporter: Automated Journalism Covering The Blue Amazon","zoom":""},{"UID":"paper17","abstract":"Visual representation of data like charts and tables can be challenging to understand for readers. Previous work showed that combining visualisations with text can improve the communication of insights in static contexts, but little is known about interactive ones. In this work we present an NLG chatbot that processes natural language queries and provides insights through a combination of charts and text. We apply it to nutrition, a domain communication quality is critical. Through crowd-sourced evaluation we compare the informativeness of our chatbot against traditional, static diet-apps. We find that the conversational context significantly improved users understanding of dietary data in various tasks, and that users considered the chatbot as more useful and quick to use than traditional apps.","authors":["Simone Balloccu","Ehud Reiter"],"code":"static/paperfiles/paper17_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326054443433984","full_video":"https://player.vimeo.com/video/728953112","paper":"static/paperfiles/paper17_paper.pdf","session":"Oral Session 1","short_time":"14:00 EST","slides":"static/paperfiles/paper17_slides.pdf","start_time":"2023-07-19T14:00:00Z","time":"Wednesday 07/19 14:00 EST","title":"Comparing Informativeness Of An NLG Chatbot Vs Graphical App In Diet-Information Domain","zoom":"https://colby.zoom.us/j/97880205507"},{"UID":"paper14","abstract":"Hallucinations and omissions need to be carefully handled when using neural models for performing Natural Language Generation tasks. In the particular case of data to text applications, neural models are usually trained on large-scale datasets and sometimes generate text including divergences with respect to the input data. In this paper, we show the impact of the lack of domain knowledge in the generation of texts containing input-output divergences through a use case on meteorology. To analyze these phenomena we adapt a Transformer-based model to our specific domain, i.e., meteorology, and train it with a new dataset and corpus curated by meteorologists. Then, we perform a divergences\u2019 detection step with a simple detector in order to identify the clearest divergences, especially those involving hallucinations. Finally, these hallucinations are analyzed by an expert in the meteorology domain, with the aim of classifying them by severity, taking into account the domain knowledge.","authors":["Javier Gonz\u00e1lez Corbelle","Alberto Bugar\u00edn-Diz","Jose Maria Alonso-Moral","Juan Taboada"],"code":"static/paperfiles/paper14_code.zip","data":"static/paperfiles/paper14_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994325759512543415","full_video":"https://player.vimeo.com/video/728953255","paper":"static/paperfiles/paper14_paper.pdf","session":"Oral Session 2","short_time":"10:50 EST","slides":"static/paperfiles/paper14_slides.pdf","start_time":"2023-07-20T10:50:00Z","summary_video":"https://player.vimeo.com/video/728953260","time":"Thursday 07/20 10:50 EST","title":"Dealing With Hallucination And Omission In Neural Natural Language Generation: A Use Case On Meteorology.","zoom":"https://colby.zoom.us/j/94311056456"},{"UID":"paper43","abstract":"We report the results of DialogSum Challenge, the shared task on summarizing real-life scenario dialogues at INLG 2023. Four teams participate in this shared task and three submit their system reports, exploring different methods to improve the performance of dialogue summarization. Although there is a great improvement over the baseline models regarding automatic evaluation metrics, such as ROUGE scores, we find that there is a salient gap between model generated outputs and human annotated summaries by human evaluation from multiple aspects. These findings demonstrate the difficulty of dialogue summarization and suggest that more fine-grained evaluatuion metrics are in need.","authors":["Naihao Deng","Yulong Chen","Yang Liu","Yue Zhang"],"code":"static/paperfiles/paper43_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994685798416916512","full_video":"https://player.vimeo.com/video/728956486","paper":"static/paperfiles/paper43_paper.pdf","session":"GenChal","short_time":"10:30 EST","slides":"static/paperfiles/paper43_slides.pdf","start_time":"2023-07-21T10:30:00Z","time":"Friday 07/21 10:30 EST","title":"Dialogsum Challenge: Results Of The Dialogue Summarization Shared Task","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper46","abstract":"","authors":["Conrad Lundberg","Leyre S\u00e1nchez Vi\u00f1uela","Siena Biales"],"discord":"https://discordapp.com/channels/991043733573218384/994686229125800090","full_video":"https://player.vimeo.com/video/728956416","paper":"static/paperfiles/paper46_paper.pdf","poster":"static/paperfiles/paper46_poster.pdf","session":"GenChal","short_time":"12:00 EST","slides":"static/paperfiles/paper46_slides.pdf","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Dialogue Summarization Using Bart","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper11","abstract":"Docket files, also known as plumitifs, are legal text documents describing judicial cases. They are present in most jurisdictions and are meant to provide a window on legal systems. They contain information of a judicial case such as parties\u2019 identities, accusations\u2019 provisions, decisions, and pleas. However, this information is cryptic, using abbreviations, and making references to the criminal code. In this paper, we explore the use of neural text generators to improve the legal accuracy of the docket file verbalization regarding the accusations, decisions, and pleas sections. We introduce a legal accuracy evaluation scale used by jurists to manually assess the performance of three architectures with different levels of prior knowledge injection. We also study the correlation of our human evaluation methodology with automatic metrics.","authors":["Nicolas Garneau","Eve Gaumond","Luc Lamontagne","Pierre-Luc D\u00e9ziel"],"code":"static/paperfiles/paper11_code.zip","data":"static/paperfiles/paper11_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994325542268584026","full_video":"https://player.vimeo.com/video/728950734","paper":"static/paperfiles/paper11_paper.pdf","session":"Oral Session 2","short_time":"10:30 EST","slides":"static/paperfiles/paper11_slides.pdf","start_time":"2023-07-20T10:30:00Z","time":"Thursday 07/20 10:30 EST","title":"Evaluating Legal Accuracy Of Neural Generators On The Generation Of Criminal Court Dockets Description","zoom":"https://colby.zoom.us/j/94311056456"},{"UID":"paper5","abstract":"For autonomous agents such as robots to effectively communicate with humans, they must be able to refer to different entities in situated contexts. In service of this goal, researchers have recently attempted to model the selection of referring forms on the basis of cognitive status (informed by Givenness Hierarchy), and have shown promising results with over 80% accuracy. However, we argue that the task environments lack ecological validity, due to their use of a small number of objects that are constantly activated and easily uniquely identifiable. Accordingly, we present a novel buildingconstruction task that we believe has increased ecological validity. We then show how training cognitive status informed referring form selection models on data collected within this novel task environment yields substantially different results from those found in previous work, providing key insights and directions for future work.","authors":["Zhao Han","Polina Rygina","Thomas Williams"],"code":"static/paperfiles/paper5_code.zip","data":"static/paperfiles/paper5_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994325127871340604","full_video":"https://player.vimeo.com/video/728950707","paper":"static/paperfiles/paper5_paper.pdf","poster":"static/paperfiles/paper5_poster.pdf","session":"Oral Session 4","short_time":"16:00 EST","slides":"static/paperfiles/paper5_slides.pdf","start_time":"2023-07-21T16:00:00Z","time":"Friday 07/21 16:00 EST","title":"Evaluating Referring Form Selection Models In Partially-Known Environments","zoom":"https://colby.zoom.us/j/99072204530"},{"UID":"paper13","abstract":"State-of-the-art image captioning models achieve very good performance in generating descriptions for instances of visual categories and reasoning about them, e.g. imposing distinctiveness of the description in the context of distractors. In this work, we propose an inference mechanism that extends an instancelevel captioning model to generate coherent and informative descriptions for groups of visual objects from the same or different categories. We test our model in the domain of bird descriptions. We show that group-level descriptions generated by our method are (i) coherent, pulling together properties that are true for all or majority of its instances, and (ii) informative, as they allow an external BERT-based text classifier to identify the target category more accurately in comparison to single-instance captions and are preferred by human evaluators.","authors":["Nazia Attari","David Schlangen","Martin Heckmann","Heiko Wersing","Sina Sina Zarrie\u00df"],"code":"static/paperfiles/paper13_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994325681259421807","full_video":"https://player.vimeo.com/video/728953287","paper":"static/paperfiles/paper13_paper.pdf","session":"Oral Session 4","short_time":"16:40 EST","slides":"static/paperfiles/paper13_slides.pdf","start_time":"2023-07-21T16:40:00Z","time":"Friday 07/21 16:40 EST","title":"Generating Coherent And Informative Descriptions For Groups Of Visual Objects And Categories: A Simple Decoding Approach","zoom":"https://colby.zoom.us/j/99072204530"},{"UID":"paper20","abstract":"We investigate the problem of generating landmark-based manipulation instructions (e.g. move the blue block so that it touches the red block on the right) from image pairs showing a before and an after state in a visual scene. We present a transformer model with difference attention heads that learns to attend to target and landmark objects in consecutive images via a difference key. Our model outperforms the state-of-the-art for instruction generation on the BLOCKS dataset and particularly improves the accuracy of generated target and landmark references. Furthermore, our model outperforms state-of-the-art models on a difference spotting dataset.","authors":["Sina Zarrie\u00df","Henrik Voigt","David Schlangen","Philipp Sadler"],"code":"static/paperfiles/paper20_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326351727304804","full_video":"https://player.vimeo.com/video/728953023","paper":"static/paperfiles/paper20_paper.pdf","session":"Oral Session 4","short_time":"17:00 EST","slides":"static/paperfiles/paper20_slides.pdf","start_time":"2023-07-21T17:00:00Z","time":"Friday 07/21 17:00 EST","title":"Generating Landmark-Based Manipulation Instructions From Image Pairs","zoom":"https://colby.zoom.us/j/99072204530"},{"UID":"paper2","abstract":"Quality management and assurance is key for space agencies to guarantee the success of space missions, which are high-risk and extremely costly. In this paper, we present a system to generate quizzes, a common resource to evaluate the effectiveness of training sessions, from documents about quality assurance procedures in the Space domain. Our system leverages state of the art auto-regressive models like T5 and BART to generate questions, and a RoBERTa model to extract answers for such questions, thus verifying their suitability.","authors":["Andres Garcia-Silva","Cristian Berrio","Jose Manuel Gomez-Perez","Jose Antonio Martinez-Heras","Patrick Fleith","Stefano Scaglioni"],"code":"static/paperfiles/paper2_code.zip","discord":"https://discordapp.com/channels/991043733573218384/996467780922654740","full_video":"https://player.vimeo.com/video/728948641","paper":"static/paperfiles/paper2_paper.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","slides":"static/paperfiles/paper2_slides.pdf","start_time":"2023-07-19T16:30:00Z","summary_video":"https://player.vimeo.com/video/728948631","time":"Wednesday 07/19 16:30 EST","title":"Generating Quizzes To Support Training On Quality Management And Assurance In Space Science And Engineering","zoom":""},{"UID":"paper18","abstract":"Asking questions during a lecture is a central part of the traditional classroom setting which benefits both students and instructors in many ways. However, no previous work has studied the task of automatically generating student questions based on explicit lecture context. We study the feasibility of automatically generating student questions given the lecture transcript windows where the questions were asked. First, we create a data set of student questions and their corresponding lecture transcript windows. Using this data set, we investigate variants of T5, a sequence-to-sequence generative language model, for a preliminary exploration of this task. Specifically, we compare the effects of training with continuous prefix tuning and pre-training with search engine queries. Question generation evaluation results on two MOOCs show that that pre-training on search engine queries tends to make the generation model more precise whereas continuous prefix tuning offers mixed results.","authors":["Kevin Ros","Maxwell Jong","Chak Ho Chan","ChengXiang Zhai"],"code":"static/paperfiles/paper18_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326186408812696","full_video":"https://player.vimeo.com/video/728953091","poster":"static/paperfiles/paper18_poster.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","slides":"static/paperfiles/paper18_slides.pdf","start_time":"2023-07-19T16:30:00Z","time":"Wednesday 07/19 16:30 EST","title":"Generation Of Student Questions For Inquiry-Based Learning","zoom":""},{"UID":"paper32","abstract":"We hosted a shared task to investigate the factors influencing the quality of the codemixed text generation systems. The teams experimented with two systems that generate synthetic code-mixed Hinglish sentences. They also experimented with human ratings that evaluate the generation quality of the two systems. The first-of-its-kind, proposed subtasks, (i) quality rating prediction and (ii) annotators\u2019 disagreement prediction of the synthetic Hinglish dataset made the shared task quite popular among the multilingual research community. A total of 46 participants comprising 23 teams from 18 institutions registered for this shared task. The detailed description of the task and the leaderboard is available at https://codalab.lisn. upsaclay.fr/competitions/1688.","authors":["Vivek Srivastava","Mayank Singh"],"code":"static/paperfiles/paper32_code.zip","data":"static/paperfiles/paper32_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994684517010899025","full_video":"https://player.vimeo.com/video/728955611","paper":"static/paperfiles/paper32_paper.pdf","session":"GenChal","short_time":"10:50 EST","slides":"static/paperfiles/paper32_slides.pdf","start_time":"2023-07-21T10:50:00Z","time":"Friday 07/21 10:50 EST","title":"Hinglisheval Generation Challenge On Quality Estimation Of Synthetic Code-Mixed Text: Overview And Results","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper36","abstract":"In this paper we describe a system submitted to the INLG 2023 Generation Challenge (GenChal) on Quality Evaluation of the LowResource Synthetically Generated Code-Mixed Hinglish Text. We implement a Bi-LSTMbased neural network model to predict the Average rating score and Disagreement score of the synthetic Hinglish dataset. In our models, we used word embeddings for English and Hindi data, and one hot encodings for Hinglish data. We achieved a F1 score of 0.11, and mean squared error of 6.0 in the average rating score prediction task. In the task of Disagreement score prediction, we achieve a F1 score of 0.18, and mean squared error of 5.0.","authors":["Prantik Guha","Rudra Dhar","Dipankar Das"],"code":"static/paperfiles/paper36_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994685110685282434","paper":"static/paperfiles/paper36_paper.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Ju_Nlp At Hinglisheval: Quality Evaluation Of The Low-Resource Code-Mixed Hinglish Text","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper19","abstract":"Question Generation (QG) receives increasing research attention in the NLP community. One QG motivation is to facilitate the preparation of educational reading practice and assessments. While significant advancement of QG techniques was reported, we find current QG techniques are short in terms of controllability and question difficulty for educational applications. This paper reports our studies toward the two issues. First, we report a state-of-the-art examlike QG model by advancing the current best model from 11.96 to 20.19 (in terms of BLEU 4 score). Second, we propose a QG model that allows users to provide keywords for gUIDing QG direction. Human evaluation and case studies are conducted to demonstrate the feasibility of controlling question generation direction.","authors":["Ying-Hong Chan","Ho-Lam Chung","Yao-Chung Fan"],"code":"static/paperfiles/paper19_code.zip","data":"static/paperfiles/paper19_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994326281632092171","full_video":"https://player.vimeo.com/video/728953033","paper":"static/paperfiles/paper19_paper.pdf","session":"Demo/Poster Session 1","short_time":"16:30 EST","slides":"static/paperfiles/paper19_slides.pdf","start_time":"2023-07-19T16:30:00Z","time":"Wednesday 07/19 16:30 EST","title":"Keyword Provision Question Generation For Facilitating Educational Reading Comprehension Preparation","zoom":""},{"UID":"paper25","abstract":"Multilingual language pretraining enables possibilities of transferring task knowledge learned from a rich-resource source language to the other, particularly favoring those low-resource languages with few or no task annotated data. However, knowledge about language and tasks encoded is strongly entangled in multilingual neural representations, thereby the learned task knowledge falsely correlated to the source language, falling short of cross-lingual transferability. In this paper, we present a novel language-agnostic finetuning (LAFT) to facilitate zero-resource cross-lingual transfer for text generation. LAFT performs language-agnostic task acquisition to isolate task learning completely from the source language, and then language specification for better generation for specified languages. Experiments demonstrate that the proposed approach facilitates a better and parameter-efficient transferability on two text generation tasks.","authors":["Xianze Wu","Zaixiang Zheng","Hao Zhou","Yong Yu"],"code":"static/paperfiles/paper25_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326644896579694","full_video":"https://player.vimeo.com/video/728954685","paper":"static/paperfiles/paper25_paper.pdf","session":"Oral Session 3","short_time":"14:20 EST","slides":"static/paperfiles/paper25_slides.pdf","start_time":"2023-07-21T14:20:00Z","time":"Friday 07/21 14:20 EST","title":"Laft: Cross-Lingual Transfer For Text Generation By Language-Agnostic Finetuning","zoom":"https://colby.zoom.us/j/98590322530"},{"UID":"paper23","abstract":"We focus on the Embodied Question Answering (EQA) task, the dataset and the models (Das et al., 2018). In particular, we examine the effects of vision perturbation at different levels by providing the model with either incongruent, black or random noise images. We observe that the model is still able to learn from general visual patterns, suggesting that they capture some common sense reasoning about the visual world. We argue that a better set of data and models are required to achieve better performance in predicting (generating) correct answers. The code is available here: https://github.com/ GU-CLASP/embodied-qa.","authors":["Nikolai Ilinykh","Yasmeen Emampoor","Simon Dobnik"],"code":"static/paperfiles/paper23_code.zip","discord":"https://discordapp.com/channels/991043733573218384/996848512421593199","full_video":"https://player.vimeo.com/video/728954773","paper":"static/paperfiles/paper23_paper.pdf","poster":"static/paperfiles/paper23_poster.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","slides":"static/paperfiles/paper23_slides.pdf","start_time":"2023-07-20T15:00:00Z","time":"Thursday 07/20 15:00 EST","title":"Look And Answer The Question: On The Role Of Vision In Embodied Question Answering","zoom":""},{"UID":"paper16","abstract":"Auto regressive text generation for lowresource languages, particularly the option of using pre-trained language models, is a relatively under-explored problem. In this paper, we model Math Word Problem (MWP) generation as an auto-regressive text generation problem. We evaluate the pre-trained sequence-tosequence language models (mBART and mT5) in the context of two low-resource languages, Sinhala and Tamil, as well as English. For the evaluation, we create a multi-way parallel MWP dataset for the considered languages. Our empirical evaluation analyses how the performance of the pre-trained models is affected by the (1) amount of language data used during pre-training, (2) amount of data used in finetuning, (3) input seed length and (4) context differences in MWPs. Our results reveal that the considered pre-trained models are capable of generating meaningful MWPs even for the languages under-represented in these models, even though the amount of fine-tuning data and seed length are small. Our human evaluation shows that a Mathematics tutor can edit a generation question fairly easily, thus highlighting the practical utility ofautomatically generating MWPs.","authors":["Kashyapa Niyarepola","Dineth Athapaththu","Savindu Kalsara Ekanayake","Surangika Ranathunga"],"code":"static/paperfiles/paper16_code.zip","data":"static/paperfiles/paper16_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994325872309960884","full_video":"https://player.vimeo.com/video/728953139","paper":"static/paperfiles/paper16_paper.pdf","poster":"static/paperfiles/paper16_poster.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","slides":"static/paperfiles/paper16_slides.pdf","start_time":"2023-07-20T15:00:00Z","time":"Thursday 07/20 15:00 EST","title":"Math Word Problem Generation With Multilingual Language Models","zoom":""},{"UID":"paper34","abstract":"This paper describes the system description for the HinglishEval challenge at INLG 2023. The goal of this task was to investigate the factors influencing the quality of the codemixed text generation system. The task was divided into two subtasks, quality rating prediction and annotators\u2019 disagreement prediction of the synthetic Hinglish dataset. We attempted to solve these tasks using sentencelevel embeddings, which are obtained from mean pooling the contextualized word embeddings for all input tokens in our text. We experimented with various classifiers on top of the embeddings produced for respective tasks. Our best-performing system ranked 1st on subtask B and 3rd on subtask A. We make our code available here: https://github. com/nikhilbyte/Hinglish-qEval.","authors":["Nikhil Singh"],"code":"static/paperfiles/paper34_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994684935711502387","paper":"static/paperfiles/paper34_paper.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Niksss At Hinglisheval: Language-Agnostic Bert-Based Contextual Embeddings With Catboost For Quality Evaluation Of The Low-Resource Synthetically Generated Code-Mixed Hinglish Text","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper22","abstract":"Nominal metaphors are frequently used in human language and have been shown to be effective in persuading, expressing emotion, and stimulating interest. This paper tackles the problem of Chinese Nominal Metaphor (NM) generation. We introduce a novel multitask framework, which jointly optimizes three tasks: NM identification, NM component identification, and NM generation. The metaphor identification module is able to perform a self-training procedure, which discovers novel metaphors from a large-scale unlabeled corpus for NM generation. The NM component identification module emphasizes components during training and conditions the generation on these NM components for more coherent results. To train the NM identification and component identification modules, we construct an annotated corpus consisting of 6.3k sentences that contain diverse metaphorical patterns. Automatic metrics show that our method can produce diverse metaphors with good readability, where 92% of them are novel metaphorical comparisons. Human evaluation shows our model significantly outperforms baselines on consistency and creativity.","authors":["Yucheng Li","Chenghua Lin","Frank Guerin"],"code":"static/paperfiles/paper22_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326448171135006","full_video":"https://player.vimeo.com/video/729628309","paper":"static/paperfiles/paper22_paper.pdf","session":"Oral Session 4","short_time":"16:20 EST","slides":"static/paperfiles/paper22_slides.pdf","start_time":"2023-07-21T16:20:00Z","summary_video":"https://player.vimeo.com/video/728954426","time":"Friday 07/21 16:20 EST","title":"Nominal Metaphor Generation With Multitask Learning","zoom":"https://colby.zoom.us/j/99072204530"},{"UID":"paper10","abstract":"We present a simple and effective way to generate a variety of paraphrases and find a good quality paraphrase among them. As in previous studies, it is difficult to ensure that one generation method always generates the best paraphrase in various domains. Therefore, we focus on finding the best candidate from multiple candidates, rather than assuming that there is only one combination of generative models and decoding options. Our approach shows that it is easy to apply in various domains and has sufficiently good performance compared to previous methods. In addition, our approach can be used for data augmentation that extends the downstream corpus, showing that it can help improve performance in English and Korean datasets.","authors":["Joosung Lee"],"code":"static/paperfiles/paper10_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994325442121170967","full_video":"https://player.vimeo.com/video/728950752","paper":"static/paperfiles/paper10_paper.pdf","poster":"static/paperfiles/paper10_poster.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","start_time":"2023-07-20T15:00:00Z","summary_video":"https://player.vimeo.com/video/728950773","time":"Thursday 07/20 15:00 EST","title":"Paraphrasing Via Ranking Many Candidates","zoom":""},{"UID":"paper9","abstract":"Pre-trained language models (PLMs) fail to generate long-form narrative text because they do not consider global structure. As a result, the generated texts are often incohesive, repetitive, or lack content. Recent work in story generation reintroduced explicit content planning in the form of prompts, keywords, or semantic frames. Trained on large parallel corpora, these models can generate more logical event sequences and thus more contentful stories. However, these intermediate representations are often not in natural language and cannot be utilized by PLMs without fine-tuning. We propose generating story plots using offthe-shelf PLMs while maintaining the benefit of content planning to generate cohesive and contentful stories. Our proposed method, SCRATCHPLOT, first prompts a PLM to compose a content plan. Then, we generate the story\u2019s body and ending conditioned on the content plan. Furthermore, we take a generateand-rank approach by using additional PLMs to rank the generated (story, ending) pairs. We benchmark our method with various baselines and achieved superior results in both human and automatic evaluation.","authors":["Yiping Jin","Vishakha Kadam","Dittaya Wanvarie"],"code":"static/paperfiles/paper9_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994325394406781038","full_video":"https://player.vimeo.com/video/728950786","paper":"static/paperfiles/paper9_paper.pdf","session":"Oral Session 2","short_time":"11:10 EST","slides":"static/paperfiles/paper9_slides.pdf","start_time":"2023-07-20T11:10:00Z","summary_video":"https://player.vimeo.com/video/728950808","time":"Thursday 07/20 11:10 EST","title":"Plot Writing From Pre-Trained Language Models","zoom":"https://colby.zoom.us/j/94311056456"},{"UID":"paper33","abstract":"Code-Mixing is a phenomenon of mixing two or more languages in a speech event and is prevalent in multilingual societies. Given the low-resource nature of Code-Mixing, machine generation of code-mixed text is a prevalent approach for data augmentation. However, evaluating the quality of such machine generated code-mixed text is an open problem. In our submission to HinglishEval, a sharedtask collocated with INLG2023, we attempt to build models factors that impact the quality of synthetically generated code-mix text by predicting ratings for code-mix quality. HinglishEval Shared Task consists of two sub-tasks - a) Quality rating prediction); b) Disagreement prediction. We leverage popular codemixed metrics and embeddings of multilingual large language models (MLLMs) as features, and train task specific MLP regression models. Our approach could not beat the baseline results. However, for Subtask-A our team ranked a close second on F-1 and Cohen\u2019s Kappa Score measures and first for Mean Squared Error measure. For Subtask-B our approach ranked third for F1 score, and first for Mean Squared Error measure.","authors":["Prashant Kodali","Tanmay Sachan","Akshay Goindani","Anmol Goel","Naman Ahuja","Manish Shrivastava","Ponnurangam Kumaraguru"],"code":"static/paperfiles/paper33_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994684674389594249","paper":"static/paperfiles/paper33_paper.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Precogiiith At Hinglisheval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper26","abstract":"The emergence of noisy medium-scale quantum devices has led to proof-of-concept applications for quantum computing in various domains. Examples include Natural Language Processing (NLP) where sentence classification experiments have been carried out, as well as procedural generation, where tasks such as geopolitical map creation, and image manipulation have been performed. We explore applications at the intersection of these two areas by designing a hybrid quantum-classical algorithm for sentence generation. Our algorithm is based on the well-known simulated annealing technique for combinatorial optimisation. An implementation is provided and used to demonstrate successful sentence generation on both simulated and real quantum hardware. A variant of our algorithm can also used for music generation. This paper aims to be self-contained, introducing all the necessary background on NLP and quantum computing along the way.","authors":["Amin Karamlou","James R Wootton","Marcel Pfaffhauser"],"code":"static/paperfiles/paper26_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326683014414468","full_video":"https://player.vimeo.com/video/728954632","paper":"static/paperfiles/paper26_paper.pdf","session":"Oral Session 2","short_time":"11:30 EST","slides":"static/paperfiles/paper26_slides.pdf","start_time":"2023-07-20T11:30:00Z","summary_video":"https://player.vimeo.com/video/728954665","time":"Thursday 07/20 11:30 EST","title":"Quantum Natural Language Generation On Near-Term Devices","zoom":"https://colby.zoom.us/j/94311056456"},{"UID":"paper39","abstract":"","authors":["Mohammad Arvan","Luis Pina","Natalie Parde"],"discord":"https://discordapp.com/channels/991043733573218384/994685416387117206","full_video":"https://player.vimeo.com/video/728955671","paper":"static/paperfiles/paper39_paper.pdf","poster":"static/paperfiles/paper39_poster.pdf","session":"GenChal","short_time":"12:00 EST","slides":"static/paperfiles/paper39_slides.pdf","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Reproducibility Of Exploring Neural Text Simplification Models: A Review","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper41","abstract":"","authors":["Maja Popovic","Sheila Castilho","Rudali HUIDrom","Anya Belz"],"discord":"https://discordapp.com/channels/991043733573218384/994685594653425664","full_video":"https://player.vimeo.com/video/728956351","paper":"static/paperfiles/paper41_paper.pdf","poster":"static/paperfiles/paper41_poster.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Reproducing A Manual Evaluation Of The Simplicity Of Text Simplification System Outputs","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper24","abstract":"Generating an argumentative conclusion from a set of textual premises is a challenging task, due to a large range of possible conclusions. In order to provide a conclusion generation model with gUIDance towards generating conclusions from a certain perspective, we explore the impact of conditioning the model on information about the desired framing. We experiment with conditioning generation via generic frame classes as well as with so-called issue-specific frames. Beyond conditioning the model on a desired frame, we investigate the impact of strategies to further improve the generated conclusion by i) an informative label smoothing method that dynamically smooths one-hot-encoded reference conclusion vectors as a regularization mechanism, and ii) a conclusion reranking strategy based on referenceless scores at inference time. We evaluate the benefits of our methods using metrics for automatic evaluation complemented with an extensive manual study. Our results show that frame-gUIDed conclusion generation is beneficial: it increases the ratio of valid and novel conclusions by 23%-points compared to a baseline without frame information. Our work indicates that i) by injecting frame information, conclusion generation can be directed towards desired aspects and ii) at the same time it can be manually confirmed to yield more valid and novel conclusions.","authors":["Philipp Heinisch","Anette Frank","Juri Opitz","Philipp Cimiano"],"code":"static/paperfiles/paper24_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326591452741783","full_video":"https://player.vimeo.com/video/728954733","paper":"static/paperfiles/paper24_paper.pdf","poster":"static/paperfiles/paper24_poster.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","slides":"static/paperfiles/paper24_slides.pdf","start_time":"2023-07-20T15:00:00Z","summary_video":"https://player.vimeo.com/video/728954752","time":"Thursday 07/20 15:00 EST","title":"Strategies For Framing Argumentative Conclusion Generation","zoom":""},{"UID":"paper44","abstract":"In this paper, we present our approach to the DialogSum challenge, which was proposed as a shared task aimed to summarize dialogues from real-life scenarios. The challenge was to design a system that can generate fluent and salient summaries of a multi-turn dialogue text. Dialogue summarization has many commercial applications as it can be used to summarize conversations between customers and service agents, meeting notes, conference proceedings etc. Appropriate dialogue summarization can enhance the experience of conversing with chatbots or personal digital assistants. We have proposed a topic-based abstractive summarization method, which is generated by fine-tuning PEGASUS1 , which is the state of the art abstractive summary generation model.We have compared different types of fine-tuning approaches that can lead to different types of summaries. We found that since conversations usually veer around a topic, using topics along with the dialoagues, helps to generate more human-like summaries. The topics in this case resemble user perspective, around which summaries are usually sought. The generated summary has been evaluated with ground truth summaries provided by the challenge owners. We use the py-rouge score and BERT-Score metrics to compare the results.","authors":["Vipul Chauhan","Prasenjeet Roy","Lipika Dey","Tushar Goel"],"code":"static/paperfiles/paper44_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994685949860647106","full_video":"https://player.vimeo.com/video/728956470","paper":"static/paperfiles/paper44_paper.pdf","poster":"static/paperfiles/paper44_poster.pdf","session":"GenChal","short_time":"12:00 EST","slides":"static/paperfiles/paper44_slides.pdf","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Tcs_Witm_2023 @ Dialogsum: Topic Oriented Summarization Using Transformer Based Encoder Decoder Model","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper6","abstract":"The recent advances in transfer learning techniques and pre-training of large contextualized encoders foster innovation in real-life applications, including dialog assistants. Practical needs of intent recognition require effective data usage and the ability to constantly update supported intents, adopting new ones, and abandoning outdated ones. In particular, the generalized zero-shot paradigm, in which the model is trained on the seen intents and tested on both seen and unseen intents, is taking on new importance. In this paper, we explore the generalized zero-shot setup for intent recognition. Following best practices for zero-shot text classification, we treat the task with a sentence pair modeling approach. We outperform previous state-of-the-art f1-measure by up to 16% for unseen intents, using intent labels and user utterances and without accessing external sources (such as knowledge bases). Further enhancement includes lexicalization of intent labels, which improves performance by up to 7%. By using task transferring from other sentence pair tasks, such as Natural Language Inference, we gain additional improvements.","authors":["Dmitry Lamanov","Pavel Burnyshev","Valentin Malykh","Andrey Bout","Irina Piontkovskaya"],"code":"static/paperfiles/paper6_code.zip","data":"static/paperfiles/paper6_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994326800081637446","full_video":"https://player.vimeo.com/video/728950889","paper":"static/paperfiles/paper6_paper.pdf","session":"Oral Session 1","short_time":"14:20 EST","slides":"static/paperfiles/paper6_slides.pdf","start_time":"2023-07-19T14:20:00Z","time":"Wednesday 07/19 14:20 EST","title":"Template-Based Approach To Zero-Shot Intent Recognition","zoom":"https://colby.zoom.us/j/97880205507"},{"UID":"paper37","abstract":"Against a background of growing interest in reproducibility in NLP and ML, and as part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP, we organised the second shared task on reproducibility of evaluations in NLG, ReproGen 2023. This paper describes the shared task, summarises results from the reproduction studies submitted, and provides further comparative analysis of the results. Out of six initial team registrations, we received submissions from five teams. Meta-analysis of the five reproduction studies revealed varying degrees of reproducibility, and allowed further tentative conclusions about what types of evaluation tend to have better reproducibility.","authors":["Anya Belz","Anastasia Shimorina","Maja Popovic","Ehud Reiter"],"code":"static/paperfiles/paper37_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994685192591638649","full_video":"https://player.vimeo.com/video/728955735","paper":"static/paperfiles/paper37_paper.pdf","session":"GenChal","short_time":"11:10 EST","slides":"static/paperfiles/paper37_slides.pdf","start_time":"2023-07-21T11:10:00Z","time":"Friday 07/21 11:10 EST","title":"The 2023 Reprogen Shared Task On Reproducibility Of Evaluations In NLG: Overview And Results","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper40","abstract":"We investigate the data collected for the Accuracy Evaluation Shared Task as a retrospective reproduction study. The shared task was based upon errors found by human annotation of computer generated summaries of basketball games. Annotation was performed in three separate stages, with texts taken from the same three systems and checked for errors by the same three annotators. We show that the mean count of errors was consistent at the highest level for each experiment, with increased variance when looking at per-system and/or per-errortype breakdowns.","authors":["Craig Thomson","Ehud Reiter"],"code":"static/paperfiles/paper40_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994685533244629176","full_video":"https://player.vimeo.com/video/728955651","paper":"static/paperfiles/paper40_paper.pdf","poster":"static/paperfiles/paper40_poster.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"The Accuracy Evaluation Shared Task As A Retrospective Reproduction Study","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper31","abstract":"We propose the shared task of cross-lingual conversation summarization, ConvSumX Challenge, opening new avenues for researchers to investigate solutions that integrate conversation summarization and machine translation. This task can be particularly useful due to the emergence of online meetings and conferences. We use a new benchmark, covering 2 real-world scenarios and 3 language directions, including a low-resource language, for evaluation. We hope that ConvSumX can motivate research to go beyond English and break the barrier for non-English speakers to benefit from recent advances of conversation summarization.","authors":["Yulong Chen","Ming Zhong","Xuefeng Bai","Naihao Deng","Jing Li","Yue Zhang"],"code":"static/paperfiles/paper31_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994684335389151372","full_video":"https://player.vimeo.com/video/729629666","paper":"static/paperfiles/paper31_paper.pdf","session":"GenChal","short_time":"11:45 EST","slides":"static/paperfiles/paper31_slides.pdf","start_time":"2023-07-21T11:45:00Z","time":"Friday 07/21 11:45 EST","title":"The Cross-Lingual Conversation Summarization Challenge","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper30","abstract":"We would host the AutoMin generation challenge at INLG 2023 as a follow-up of the first AutoMin shared task at Interspeech 2021. Our shared task primarily concerns the automated generation of meeting minutes from multi-party meeting transcripts. In our first venture, we observed the difficulty of the task and highlighted a number of open problems for the community to discuss, attempt, and solve. Hence, we invite the Natural Language Generation (NLG) community to take part in the second iteration of AutoMin. Like the first, the second AutoMin will feature both English and Czech meetings and the core task of summarizing the manuallyrevised transcripts into bulleted minutes. A new challenge we are introducing this year is to devise efficient metrics for evaluating the quality of minutes. We will also host an optional track to generate minutes for European parliamentary sessions. We carefully curated the datasets for the above tasks. Our ELITR Minuting Corpus has been recently accepted to LREC 2023 and publicly released.1 We are already preparing a new test set for evaluating the new shared tasks. We hope to carry forward the learning from the first AutoMin and instigate more community attention and interest in this timely yet challenging problem. INLG, the premier forum for the NLG community, would be an appropriate venue to discuss the challenges and future of Automatic Minuting. The main objective of the AutoMin GenChal at INLG 2023 would be to come up with efficient methods to automatically generate meeting minutes and design evaluation metrics to measure the quality of the minutes.","authors":["Tirthankar Ghosal","Marie Hled\u00edkov\u00e1","Muskaan Singh","Anna Nedoluzhko","Ondrej Bojar"],"code":"static/paperfiles/paper30_code.zip","data":"static/paperfiles/paper30_data.zip","discord":"https://discordapp.com/channels/991043733573218384/994683942701641749","paper":"static/paperfiles/paper30_paper.pdf","session":"GenChal","short_time":"11:30 EST","start_time":"2023-07-21T11:30:00Z","time":"Friday 07/21 11:30 EST","title":"The Second Automatic Minuting (Automin) Challenge: Generating And Evaluating Minutes From Multi-Party Meetings","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper4","abstract":"We present a free online demo of THEaiTRobot, an open-source bilingual tool for interactively generating theatre play scripts, in two versions. THEaiTRobot 1.0 uses the GPT-2 language model with minimal adjustments. THEaiTRobot 2.0 uses two models created by fine-tuning GPT-2 on purposefully collected and processed datasets and several other components, generating play scripts in a hierarchical fashion (title $\rightarrow$ synopsis $\rightarrow$ script). The underlying tool is used in the THEaiTRE project to generate scripts for plays, which are then performed on stage by a professional theatre.","authors":["Rudolf Rosa","Patr\u00edcia Schmidtov\u00e1","Alisa Zakhtarenko","Ondrej Dusek","Tom\u00e1\u0161 Musil","David Mare\u010dek","Saad Obaid Ul Islam","Marie Nov\u00e1kov\u00e1","Kl\u00e1ra Voseck\u00e1","Daniel Hrbek","David Ko\u0161\u0165\u00e1k"],"code":"static/paperfiles/paper4_code.zip","discord":"https://discordapp.com/channels/991043733573218384/996467903161454694","full_video":"https://player.vimeo.com/video/728948556","paper":"static/paperfiles/paper4_paper.pdf","session":"Demo/Poster Session 2","short_time":"15:00 EST","slides":"static/paperfiles/paper4_slides.pdf","start_time":"2023-07-20T15:00:00Z","time":"Thursday 07/20 15:00 EST","title":"Theaitrobot: An Interactive Tool For Generating Theatre Play Scripts","zoom":""},{"UID":"paper28","abstract":"Recent research in the field of conversational AI has emphasized the need for standardization of the metrics used in evaluation. In this work, we focus on evaluation methods used for multi-party dialogue systems. We present an expanded taxonomy focusing on multi-party dialogue based on the need for evaluation dimensions that address challenges associated with the presence of multiple participants. We also survey the evaluation metrics utilized in current multi-party dialogue research, and present our findings with regards to inconsistencies within existing work. Furthermore, we discuss the subsequent need to have more consistent evaluation methodologies and benchmarks. We motivate how consistency will contribute towards a better understanding of progress in the field of multi-party dialogue systems.","authors":["Khyati Mahajan","Sashank Santhanam","Samira Shaikh"],"code":"static/paperfiles/paper28_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326800081637446","full_video":"https://player.vimeo.com/video/728954548","paper":"static/paperfiles/paper28_paper.pdf","session":"Oral Session 1","short_time":"14:40 EST","slides":"static/paperfiles/paper28_slides.pdf","start_time":"2023-07-19T14:40:00Z","time":"Wednesday 07/19 14:40 EST","title":"Towards Evaluation Of Multi-Party Dialogue Systems","zoom":"https://colby.zoom.us/j/97880205507"},{"UID":"paper38","abstract":"","authors":["Rudali HUIDrom","Ondrej Dusek","Zden\u011bk Kasner","Thiago Castro Ferreira","Anya Belz"],"discord":"https://discordapp.com/channels/991043733573218384/996858160595882074","full_video":"https://player.vimeo.com/video/728955715","paper":"static/paperfiles/paper38_paper.pdf","poster":"static/paperfiles/paper38_poster.pdf","session":"GenChal","short_time":"12:00 EST","start_time":"2023-07-21T12:00:00Z","time":"Friday 07/21 12:00 EST","title":"Two Reproductions Of A Human-Assessed Comparative Evaluation Of A Semantic Error Detection System","zoom":"https://colby.zoom.us/j/98600533576"},{"UID":"paper21","abstract":"Typologically diverse languages offer systems of lexical and grammatical aspect that allow speakers to focus on facets of event structure in ways that comport with the specific communicative setting and discourse constraints they face. In this paper, we look specifically at captions of images across Arabic, Chinese, Farsi, German, Russian, and Turkish and describe a computational model for predicting lexical aspects. Despite the heterogeneity of these languages, and the salient invocation of distinctive linguistic resources across their caption corpora, speakers of these languages show surprising similarities in the ways they frame image content. We leverage this observation for zero-shot cross-lingual learning and show that lexical aspects can be predicted for a given language despite not having observed any annotated data for this language at all.","authors":["Malihe Alikhani","Thomas H Kober","Bashar Alhafni","Yue Chen","Mert Inan","Elizabeth Kaye Nielsen","Shahab Raji","Mark Steedman","Matthew Stone"],"code":"static/paperfiles/paper21_code.zip","discord":"https://discordapp.com/channels/991043733573218384/994326403917025290","full_video":"https://player.vimeo.com/video/729841361","paper":"static/paperfiles/paper21_paper.pdf","session":"Oral Session 3","short_time":"14:00 EST","slides":"static/paperfiles/paper21_slides.pdf","start_time":"2023-07-21T14:00:00Z","summary_video":"https://player.vimeo.com/video/729841279","time":"Friday 07/21 14:00 EST","title":"Zero-Shot Cross-Linguistic Learning Of Event Semantics","zoom":"https://colby.zoom.us/j/98590322530"}]
